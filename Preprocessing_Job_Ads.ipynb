{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Job Text\n",
    "\n",
    "* pandas\n",
    "* re\n",
    "* numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk.data\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from __future__ import division\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from collections import defaultdict\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examine the data folder, including the categories and job advertisment txt documents, etc. Explain your findings here, e.g., number of folders and format of txt files, etc.\n",
    "- Load the data into proper data structures and get it ready for processing.\n",
    "- Extract webIndex and description into proper data structures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Code to inspect the provided data file...\n",
    "from sklearn.datasets import load_files # for loading multiple files\n",
    "text = load_files(r\"data/\") # load all files in the directory named data\n",
    "text.keys() # inspect the keys of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accounting_Finance', 'Engineering', 'Healthcare_Nursing', 'Sales']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.target_names # categories of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating a corpus / one document \n",
    "- Get directory paths\n",
    "- Create lists of id and texts\n",
    "- Load in as one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = [\"./data/Accounting_Finance/\", \"./data/Engineering/\", \"./data/Healthcare_Nursing/\", \"./data/Sales/\"]\n",
    "article_ids = [] # list to store the article ID\n",
    "article_txts = [] # list to store the raw text\n",
    "for i in range(len(dir_path)):\n",
    "    for filename in sorted(os.listdir(dir_path[i])): # we want to load articles in ascending order of their file names\n",
    "        if filename.endswith(\".txt\"): # we only look at the txt file\n",
    "            article_ids.append(filename.split(\".\")[0]) # split the file name with '.', \n",
    "                                                        # so the first part is the article ID, and 2nd part is 'txt'\n",
    "                                                        # we then take the first part and store it\n",
    "            path = os.path.join(dir_path[i],filename) # this gives the file path, e.g., './articles/0001.txt'\n",
    "            with open(path,\"r\",encoding= 'unicode_escape') as f: # open the txt file\n",
    "                article_txts.append(f.read()) # read the file into a string, and append it to the article_txts list\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article ID: Job_00239\n",
      "Article txt:\n",
      " Title: Pensions Administrator\n",
      "Webindex: 71852020\n",
      "Company: Hillman Saunders\n",
      "Description: Are you a proven Pensions Administrator looking to work for one of the leading third party administrators in the UK? Then I have an excellent opportunity for you. The primary responsibility will be to provide a full pension's administration service to clients and customers in an accurate, efficient and timely manner. This would include the processing and settling of transfers and leavers of the pension scheme, as well as dealing with all types of retirement queries, including quotations, settlements and manual calculations. To be considered for this role you NEED to have good working knowledge of the above pension's administration duties; this will preferably come from working in a third party pension administrator. You will also need to have a high level of numeracy and literacy as this may be tested at interview stage. In return you will receive a very competitive salary, 25 days holiday, pension and benefits package. This is an excellent opportunity not be missed, so APPLY NOW  Fusion People are committed to promoting equal opportunities to people regardless of age, gender, religion, belief, race, sexuality or disability. We operate as an employment agency and employment business. This job was originally posted as www.totaljobs.com/JobSeeking/PensionsAdministrator_job****\n"
     ]
    }
   ],
   "source": [
    "print(\"Article ID:\", article_ids[7]) # display the article ID of the 7th index\n",
    "print(\"Article txt:\\n\", article_txts[7]) # display the text of the 7th index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeRawData(article):\n",
    "    # extract the description text from the article  \n",
    "    \n",
    "    # Matching the pattern \"Description: \" and then any character after it until the end of the line \n",
    "    match = re.search(r\"Description: (.+)\", article)  \n",
    "    if match:\n",
    "      # extract the description text from the article\n",
    "      description_text = match.group(1)\n",
    "    \n",
    "    nl_article = description_text.lower() # cover all words to lowercase\n",
    "\n",
    "    pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Za-z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      #| \\w*[\\$Â£]?(?:\\d+(?:,\\d+)?)+(?:\\.\\d+)?%?\\w*  # numbers, currency and percentages, e.g. $12.40, 82%\n",
    "      | [A-Za-z]+(?:[-'][A-Za-z]*)?        # words with optional internal hyphens and apostrophes\n",
    "    '''\n",
    "    # create a tokenizer that matches the regex pattern from the assignment\n",
    "    tokenizer = nltk.RegexpTokenizer(pattern) \n",
    "    # tokenize the article\n",
    "    tokenised_article = tokenizer.tokenize(nl_article)\n",
    "    return tokenised_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Creating a list of tokenized articles\n",
    "tokenized_articles = [tokenizeRawData(article) for article in article_txts] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'you',\n",
       " 'a',\n",
       " 'proven',\n",
       " 'pensions',\n",
       " 'administrator',\n",
       " 'looking',\n",
       " 'to',\n",
       " 'work',\n",
       " 'for',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'leading',\n",
       " 'third',\n",
       " 'party',\n",
       " 'administrators',\n",
       " 'in',\n",
       " 'the',\n",
       " 'uk',\n",
       " 'then',\n",
       " 'i',\n",
       " 'have',\n",
       " 'an',\n",
       " 'excellent',\n",
       " 'opportunity',\n",
       " 'for',\n",
       " 'you',\n",
       " 'the',\n",
       " 'primary',\n",
       " 'responsibility',\n",
       " 'will',\n",
       " 'be',\n",
       " 'to',\n",
       " 'provide',\n",
       " 'a',\n",
       " 'full',\n",
       " \"pension's\",\n",
       " 'administration',\n",
       " 'service',\n",
       " 'to',\n",
       " 'clients',\n",
       " 'and',\n",
       " 'customers',\n",
       " 'in',\n",
       " 'an',\n",
       " 'accurate',\n",
       " 'efficient',\n",
       " 'and',\n",
       " 'timely',\n",
       " 'manner',\n",
       " 'this',\n",
       " 'would',\n",
       " 'include',\n",
       " 'the',\n",
       " 'processing',\n",
       " 'and',\n",
       " 'settling',\n",
       " 'of',\n",
       " 'transfers',\n",
       " 'and',\n",
       " 'leavers',\n",
       " 'of',\n",
       " 'the',\n",
       " 'pension',\n",
       " 'scheme',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'dealing',\n",
       " 'with',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'retirement',\n",
       " 'queries',\n",
       " 'including',\n",
       " 'quotations',\n",
       " 'settlements',\n",
       " 'and',\n",
       " 'manual',\n",
       " 'calculations',\n",
       " 'to',\n",
       " 'be',\n",
       " 'considered',\n",
       " 'for',\n",
       " 'this',\n",
       " 'role',\n",
       " 'you',\n",
       " 'need',\n",
       " 'to',\n",
       " 'have',\n",
       " 'good',\n",
       " 'working',\n",
       " 'knowledge',\n",
       " 'of',\n",
       " 'the',\n",
       " 'above',\n",
       " \"pension's\",\n",
       " 'administration',\n",
       " 'duties',\n",
       " 'this',\n",
       " 'will',\n",
       " 'preferably',\n",
       " 'come',\n",
       " 'from',\n",
       " 'working',\n",
       " 'in',\n",
       " 'a',\n",
       " 'third',\n",
       " 'party',\n",
       " 'pension',\n",
       " 'administrator',\n",
       " 'you',\n",
       " 'will',\n",
       " 'also',\n",
       " 'need',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'high',\n",
       " 'level',\n",
       " 'of',\n",
       " 'numeracy',\n",
       " 'and',\n",
       " 'literacy',\n",
       " 'as',\n",
       " 'this',\n",
       " 'may',\n",
       " 'be',\n",
       " 'tested',\n",
       " 'at',\n",
       " 'interview',\n",
       " 'stage',\n",
       " 'in',\n",
       " 'return',\n",
       " 'you',\n",
       " 'will',\n",
       " 'receive',\n",
       " 'a',\n",
       " 'very',\n",
       " 'competitive',\n",
       " 'salary',\n",
       " 'days',\n",
       " 'holiday',\n",
       " 'pension',\n",
       " 'and',\n",
       " 'benefits',\n",
       " 'package',\n",
       " 'this',\n",
       " 'is',\n",
       " 'an',\n",
       " 'excellent',\n",
       " 'opportunity',\n",
       " 'not',\n",
       " 'be',\n",
       " 'missed',\n",
       " 'so',\n",
       " 'apply',\n",
       " 'now',\n",
       " 'fusion',\n",
       " 'people',\n",
       " 'are',\n",
       " 'committed',\n",
       " 'to',\n",
       " 'promoting',\n",
       " 'equal',\n",
       " 'opportunities',\n",
       " 'to',\n",
       " 'people',\n",
       " 'regardless',\n",
       " 'of',\n",
       " 'age',\n",
       " 'gender',\n",
       " 'religion',\n",
       " 'belief',\n",
       " 'race',\n",
       " 'sexuality',\n",
       " 'or',\n",
       " 'disability',\n",
       " 'we',\n",
       " 'operate',\n",
       " 'as',\n",
       " 'an',\n",
       " 'employment',\n",
       " 'agency',\n",
       " 'and',\n",
       " 'employment',\n",
       " 'business',\n",
       " 'this',\n",
       " 'job',\n",
       " 'was',\n",
       " 'originally',\n",
       " 'posted',\n",
       " 'as',\n",
       " 'www',\n",
       " 'totaljobs',\n",
       " 'com',\n",
       " 'jobseeking',\n",
       " 'pensionsadministrator',\n",
       " 'job']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing to see if the tokenization worked\n",
    "tokenized_articles[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to show term frequency throughout each preprocessing step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 8309),\n",
       " ('the', 6487),\n",
       " ('to', 6265),\n",
       " ('a', 4699),\n",
       " ('of', 4630),\n",
       " ('in', 3290),\n",
       " ('for', 2832),\n",
       " ('with', 2306),\n",
       " ('will', 2021),\n",
       " ('you', 2011),\n",
       " ('be', 1869),\n",
       " ('is', 1793),\n",
       " ('as', 1425),\n",
       " ('this', 1393),\n",
       " ('an', 1361),\n",
       " ('are', 1332),\n",
       " ('experience', 1276),\n",
       " ('on', 1216),\n",
       " ('have', 1114),\n",
       " ('or', 1088)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a frequency distribution of the words\n",
    "def showTerms():\n",
    "    # Creating a list of all the words in the articles\n",
    "    word_list = list(chain.from_iterable(tokenized_articles))\n",
    "    # Creating a set of all the words in the articles\n",
    "    vocab = set(word_list)\n",
    "    # Creating a frequency distribution of the words\n",
    "    term_fd = FreqDist(word_list) \n",
    "    return term_fd\n",
    "term_fd = showTerms()\n",
    "term_fd.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing words with < 2 characters\n",
    "- #4 on Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 8309),\n",
       " ('the', 6487),\n",
       " ('to', 6265),\n",
       " ('of', 4630),\n",
       " ('in', 3290),\n",
       " ('for', 2832),\n",
       " ('with', 2306),\n",
       " ('will', 2021),\n",
       " ('you', 2011),\n",
       " ('be', 1869),\n",
       " ('is', 1793),\n",
       " ('as', 1425),\n",
       " ('this', 1393),\n",
       " ('an', 1361),\n",
       " ('are', 1332),\n",
       " ('experience', 1276),\n",
       " ('on', 1216),\n",
       " ('have', 1114),\n",
       " ('or', 1088),\n",
       " ('sales', 1030)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a variable that contains all the words in the articles\n",
    "all_words = list(chain.from_iterable(tokenized_articles))\n",
    "\n",
    "def removeLessThanTwoWords(article):\n",
    "    return [w for w in article if len(w)>=2]\n",
    "\n",
    "tokenized_articles = [removeLessThanTwoWords(article) for article in tokenized_articles]\n",
    "term_fd = showTerms()\n",
    "term_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing words from stopwords_en.txt\n",
    "- #5 on Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 1276),\n",
       " ('sales', 1030),\n",
       " ('role', 946),\n",
       " ('work', 861),\n",
       " ('business', 832),\n",
       " ('team', 789),\n",
       " ('working', 719),\n",
       " ('job', 688),\n",
       " ('care', 675),\n",
       " ('skills', 669),\n",
       " ('company', 614),\n",
       " ('client', 594),\n",
       " ('management', 572),\n",
       " ('manager', 517),\n",
       " ('support', 501),\n",
       " ('uk', 496),\n",
       " ('service', 480),\n",
       " ('excellent', 455),\n",
       " ('development', 430),\n",
       " ('required', 399)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "\n",
    "# filter out stop words\n",
    "tokenized_articles = [[w for w in article if w not in stopwords] for article in tokenized_articles]\n",
    "term_fd = showTerms()\n",
    "term_fd.most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing Words that appear once in tokenized_articles\n",
    "- #6 on assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 1276),\n",
       " ('sales', 1030),\n",
       " ('role', 946),\n",
       " ('work', 861),\n",
       " ('business', 832),\n",
       " ('team', 789),\n",
       " ('working', 719),\n",
       " ('job', 688),\n",
       " ('care', 675),\n",
       " ('skills', 669),\n",
       " ('company', 614),\n",
       " ('client', 594),\n",
       " ('management', 572),\n",
       " ('manager', 517),\n",
       " ('support', 501),\n",
       " ('uk', 496),\n",
       " ('service', 480),\n",
       " ('excellent', 455),\n",
       " ('development', 430),\n",
       " ('required', 399)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate term frequency for all words across all articles\n",
    "term_freq = FreqDist(chain.from_iterable(tokenized_articles))\n",
    "\n",
    "# Identify words that appear once\n",
    "less_freq_words_term = set(word for word, freq in term_freq.items() if freq == 1)\n",
    "\n",
    "# Remove these words from the articles\n",
    "tokenized_articles = [[word for word in article if word not in less_freq_words_term] for article in tokenized_articles]\n",
    "term_fd = showTerms()\n",
    "term_fd.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing top 50 Document Frequency Words\n",
    "- #7 on Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('care', 675),\n",
       " ('design', 337),\n",
       " ('engineering', 336),\n",
       " ('customer', 335),\n",
       " ('home', 291),\n",
       " ('ensure', 290),\n",
       " ('engineer', 285),\n",
       " ('financial', 279),\n",
       " ('staff', 271),\n",
       " ('systems', 267),\n",
       " ('time', 254),\n",
       " ('quality', 250),\n",
       " ('key', 244),\n",
       " ('requirements', 239),\n",
       " ('opportunities', 238),\n",
       " ('project', 236),\n",
       " ('environment', 235),\n",
       " ('career', 235),\n",
       " ('candidates', 233),\n",
       " ('nursing', 228)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate document frequency for each word\n",
    "doc_freq = defaultdict(int)\n",
    "\n",
    "for article in tokenized_articles:\n",
    "    # Get the unique words per article\n",
    "    for word in set(article):  \n",
    "        doc_freq[word] += 1\n",
    "\n",
    "# Get the top 50 words based on document frequency\n",
    "top_50_doc_freq_words = sorted(doc_freq, key=doc_freq.get, reverse=True)[:50]\n",
    "\n",
    "# Remove these words from each article\n",
    "tokenized_articles = [[word for word in article if word not in top_50_doc_freq_words] for article in tokenized_articles]\n",
    "term_fd = showTerms()\n",
    "term_fd.most_common(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lterm_fd = list(term_fd.keys())\n",
    "lterm_fd.sort()\n",
    "sorted_term_fd = {i: term_fd[i] for i in lterm_fd}\n",
    "with open(\"vocab.txt\", \"w\") as f:\n",
    "    # Loop through each term and its index in all_terms\n",
    "    for index, term in enumerate(sorted_term_fd):\n",
    "        # Write the term and index to the file, followed by a newline\n",
    "        f.write(f\"{term}:{index}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessed Descriptions and the Original Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a csv of the tokenized articles\n",
    "with open('tokenized_articles.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenized_articles, f)   \n",
    "\n",
    "with open('jobsBlob.pkl', 'wb') as f:\n",
    "    pickle.dump(text, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
